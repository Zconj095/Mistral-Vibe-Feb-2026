from __future__ import annotations

import json
import sys
import urllib.error
import urllib.request
from pathlib import Path
from typing import TYPE_CHECKING, ClassVar

from pydantic import BaseModel, Field

from vibe.core.tools.base import (
    BaseTool,
    BaseToolConfig,
    BaseToolState,
    ToolError,
    ToolPermission,
)
from vibe.core.tools.ui import ToolCallDisplay, ToolResultDisplay, ToolUIData

if TYPE_CHECKING:
    from vibe.core.types import ToolCallEvent, ToolResultEvent


DEFAULT_PROMPT_TEMPLATE = """### MEMORY STRUCTURE MODE (OPT-IN)
Work with memory locations, allocation speeds, categorization, and linkage.
- Identify location anchors, speed constraints, and linkage cues.
- Provide concise guidance with confidence notes.
- Keep reasoning concise; avoid revealing chain-of-thought.

Focus: {focus}
Show steps: {show_steps}
Max results: {max_results}
"""

TOOL_PROMPT = (
    "Use memory_categorization_version for memory locations, allocation speed, categorization, or linkage. "
    "Provide prompt or messages, and optionally set ocus, show_steps, and max_results."
)


class MemoryStructureMessage(BaseModel):
    role: str
    content: str


class MemoryStructureArgs(BaseModel):
    prompt: str | None = Field(default=None, description="User prompt to solve.")
    messages: list[MemoryStructureMessage] | None = Field(
        default=None, description="Optional chat messages."
    )
    system_prompt: str | None = Field(default=None, description="Optional system prompt prefix.")
    focus: str | None = Field(default=None, description="Focus label or domain.")
    show_steps: bool | None = Field(default=None, description="Whether to include a step outline.")
    max_results: int | None = Field(default=None, description="Maximum results in the outline.")
    llm_api_base: str | None = Field(default=None, description="OpenAI-compatible API base URL.")
    llm_model: str | None = Field(default=None, description="LLM model name.")
    llm_temperature: float = Field(default=0.2, description="LLM temperature.")
    llm_max_tokens: int = Field(default=900, description="LLM max tokens.")
    llm_stream: bool = Field(default=False, description="Stream LLM tokens.")


class MemoryStructureResult(BaseModel):
    answer: str
    system_prompt: str
    messages: list[MemoryStructureMessage]
    used_focus: str
    show_steps: bool
    max_results: int
    template_source: str
    warnings: list[str]
    errors: list[str]
    llm_model: str


class MemoryStructureConfig(BaseToolConfig):
    permission: ToolPermission = ToolPermission.ASK
    llm_api_base: str = Field(default="http://127.0.0.1:11434/v1", description="OpenAI-compatible API base URL.")
    llm_model: str = Field(default="gpt-oss:latest", description="Default LLM model name.")
    default_focus: str = Field(
        default="Memory categorization.",
        description="Default focus label.",
    )
    default_show_steps: bool = Field(default=False, description="Default for show_steps.")
    default_max_results: int = Field(default=4, description="Default max results.")
    prompt_path: Path | None = Field(
        default=Path.home()
        / "mistral-vibe"
        / "vibe"
        / "core"
        / "prompts"
        / "memory_categorization_version.md",
        description="Optional path to a prompt template.",
    )
    prompt_max_chars: int = Field(default=8000, description="Maximum template characters to load.")


class MemoryStructureState(BaseToolState):
    pass


class MemoryCategorizationVersion(
    BaseTool[
        MemoryStructureArgs,
        MemoryStructureResult,
        MemoryStructureConfig,
        MemoryStructureState,
    ],
    ToolUIData[MemoryStructureArgs, MemoryStructureResult],
):
    description: ClassVar[str] = "Memory categorization."

    @classmethod
    def get_tool_prompt(cls) -> str | None:
        return TOOL_PROMPT

    async def run(self, args: MemoryStructureArgs) -> MemoryStructureResult:
        warnings: list[str] = []
        errors: list[str] = []

        focus = (args.focus or self.config.default_focus).strip() or "memory structures"
        show_steps = args.show_steps if args.show_steps is not None else self.config.default_show_steps
        max_results = args.max_results if args.max_results is not None else self.config.default_max_results
        if max_results <= 0:
            raise ToolError("max_results must be positive.")

        self._validate_llm_settings(args)
        template, template_source = self._load_template(warnings)
        system_prompt = self._build_system_prompt(template, focus, show_steps, max_results, args.system_prompt)
        messages = self._normalize_messages(args, system_prompt)
        answer = self._call_llm(messages, args)

        return MemoryStructureResult(
            answer=answer,
            system_prompt=system_prompt,
            messages=messages,
            used_focus=focus,
            show_steps=show_steps,
            max_results=max_results,
            template_source=template_source,
            warnings=warnings,
            errors=errors,
            llm_model=self._resolve_model(args),
        )

    def _validate_llm_settings(self, args: MemoryStructureArgs) -> None:
        if args.llm_temperature < 0:
            raise ToolError("llm_temperature cannot be negative.")
        if args.llm_max_tokens <= 0:
            raise ToolError("llm_max_tokens must be positive.")

    def _load_template(self, warnings: list[str]) -> tuple[str, str]:
        template = DEFAULT_PROMPT_TEMPLATE
        source = "embedded"
        if not self.config.prompt_path:
            return self._truncate_template(template, warnings), source
        path = self._resolve_prompt_path(self.config.prompt_path)
        if not path.exists():
            warnings.append(f"Prompt template not found: {path}")
            return self._truncate_template(template, warnings), source
        if path.is_dir():
            warnings.append(f"Prompt template is a directory: {path}")
            return self._truncate_template(template, warnings), source
        try:
            text = path.read_text("utf-8", errors="ignore").strip()
        except OSError as exc:
            warnings.append(f"Failed to read prompt template: {exc}")
            return self._truncate_template(template, warnings), source
        if not text:
            warnings.append(f"Prompt template empty: {path}")
            return self._truncate_template(template, warnings), source
        return self._truncate_template(text, warnings), str(path)

    def _resolve_prompt_path(self, raw_path: Path | str) -> Path:
        path = Path(raw_path).expanduser()
        if not path.is_absolute():
            path = self.config.effective_workdir / path
        return path.resolve()

    def _truncate_template(self, template: str, warnings: list[str]) -> str:
        max_chars = self.config.prompt_max_chars
        if max_chars > 0 and len(template) > max_chars:
            warnings.append("Prompt template truncated to prompt_max_chars.")
            return template[:max_chars].rstrip()
        return template

    def _build_system_prompt(self, template: str, focus: str, show_steps: bool, max_results: int, prefix: str | None) -> str:
        show_steps_text = "yes" if show_steps else "no"
        rendered = self._render_template(template, focus, show_steps_text, max_results)
        if prefix and prefix.strip():
            return f"{prefix.strip()}\n\n{rendered}".strip()
        return rendered.strip()

    def _render_template(self, template: str, focus: str, show_steps_text: str, max_results: int) -> str:
        had_placeholders = any(token in template for token in ("{focus}", "{show_steps}", "{max_results}"))
        rendered = template.replace("{focus}", focus)
        rendered = rendered.replace("{show_steps}", show_steps_text)
        rendered = rendered.replace("{max_results}", str(max_results))
        if had_placeholders:
            return rendered
        extra = f"Focus: {focus}\nShow steps: {show_steps_text}\nMax results: {max_results}"
        return f"{rendered.rstrip()}\n\n{extra}"

    def _normalize_messages(self, args: MemoryStructureArgs, system_prompt: str) -> list[MemoryStructureMessage]:
        messages: list[MemoryStructureMessage] = []
        if args.messages:
            for msg in args.messages:
                role = (msg.role or "").strip() or "user"
                content = (msg.content or "").strip()
                if content:
                    messages.append(MemoryStructureMessage(role=role, content=content))
        elif args.prompt and args.prompt.strip():
            messages.append(MemoryStructureMessage(role="user", content=args.prompt.strip()))
        else:
            raise ToolError("Provide prompt or messages.")
        if not messages:
            raise ToolError("No usable messages provided.")
        if system_prompt.strip():
            messages.insert(0, MemoryStructureMessage(role="system", content=system_prompt.strip()))
        return messages

    def _call_llm(self, messages: list[MemoryStructureMessage], args: MemoryStructureArgs) -> str:
        api_base = (args.llm_api_base or self.config.llm_api_base).rstrip("/")
        url = api_base + "/chat/completions"
        payload = {
            "model": self._resolve_model(args),
            "messages": [msg.model_dump() for msg in messages],
            "temperature": args.llm_temperature,
            "max_tokens": args.llm_max_tokens,
            "stream": bool(args.llm_stream),
        }
        data = json.dumps(payload).encode("utf-8")
        req = urllib.request.Request(url, data=data, headers={"Content-Type": "application/json"})
        try:
            resp = urllib.request.urlopen(req, timeout=600)
        except urllib.error.URLError as exc:
            raise ToolError(f"LLM request failed: {exc}") from exc
        if not args.llm_stream:
            body = resp.read().decode("utf-8")
            try:
                parsed = json.loads(body)
            except json.JSONDecodeError as exc:
                raise ToolError(f"LLM response parse failed: {exc}") from exc
            return parsed["choices"][0]["message"].get("content", "").strip()
        return self._read_streaming_response(resp)

    def _read_streaming_response(self, resp) -> str:
        parts: list[str] = []
        for raw in resp:
            line = raw.decode("utf-8").strip()
            if not line:
                continue
            if line.startswith("data:"):
                line = line[len("data:") :].strip()
            if line == "[DONE]":
                break
            try:
                chunk = json.loads(line)
            except json.JSONDecodeError:
                continue
            choice = chunk.get("choices", [{}])[0]
            delta = choice.get("delta") or choice.get("message") or {}
            content = delta.get("content")
            if content:
                parts.append(content)
                sys.stdout.write(content)
                sys.stdout.flush()
        if parts:
            sys.stdout.write("\n")
            sys.stdout.flush()
        return "".join(parts).strip()

    def _resolve_model(self, args: MemoryStructureArgs) -> str:
        return args.llm_model or self.config.llm_model

    @classmethod
    def get_call_display(cls, event: ToolCallEvent) -> ToolCallDisplay:
        if not isinstance(event.args, MemoryStructureArgs):
            return ToolCallDisplay(summary="memory_categorization_version")
        return ToolCallDisplay(
            summary="memory_categorization_version",
            details={
                "focus": event.args.focus,
                "show_steps": event.args.show_steps,
                "max_results": event.args.max_results,
            },
        )

    @classmethod
    def get_result_display(cls, event: ToolResultEvent) -> ToolResultDisplay:
        if not isinstance(event.result, MemoryStructureResult):
            return ToolResultDisplay(success=False, message=event.error or event.skip_reason or "No result")
        message = "Memory structure processing complete"
        if event.result.errors:
            message = "Memory structure processing finished with errors"
        return ToolResultDisplay(
            success=not bool(event.result.errors),
            message=message,
            warnings=event.result.warnings,
            details={
                "answer": event.result.answer,
                "used_focus": event.result.used_focus,
                "show_steps": event.result.show_steps,
                "max_results": event.result.max_results,
                "template_source": event.result.template_source,
                "errors": event.result.errors,
            },
        )

    @classmethod
    def get_status_text(cls) -> str:
        return "Memory structure processing complete"
